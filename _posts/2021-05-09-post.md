---
title:  "국회뉴스 ON 사이트 크롤링"
date:   2021-05-09
categories: [python] 
---

국회뉴스 ON 사이트에서 뉴스기사 크롤링하기

[국회뉴스 ON 사이트][국회뉴스 ON 사이트] 에서 게시된 기사들을 제목, 기사 날짜, 본문, url을 뽑아 엑셀에 저장하는 작업
해당 사이트는 페이지 소스로 확인해봤을 때, 기사가 숨겨져 있어 beautifulSoup만으로는 기사 크롤링이 어려웠다.

따라서 처음에는 selenium을 이용한 동적 웹크롤링을 통해 기사 url을 데이터 프레임에 저장한 후, 크롤링한 href 주소를 이용해 자세한 내용들을 크롤링했다.

사이트에 들어가면 확인할 수 있다시피, 기사는 크게 top 뉴스와 bottom 뉴스로 나뉘어져 있었다.

top 뉴스는 페이지가 넘어가도 변하지 않길래 한번만 크롤링작업을 거쳤다.

```python
import requests
from bs4 import BeautifulSoup
import os
from selenium import webdriver

if __name__ == '__main__':
        
        driver = webdriver.Chrome("chrome driver 설치 위치")
                 
        href = list()
        
        for n in range(1, 원하는 만큼):
            list_1 = []
            list_2 = []
            url = "http://www.naon.go.kr/assemblyNews/naon_today/list.do?pageNo="+str(n)
            driver.get(url) 
            driver.implicitly_wait(5)
            if(n == 1) : list_1 = driver.find_element_by_class_name("news_list") # top 뉴스
            list_2 = driver.find_element_by_class_name("tit_list")
            
            if(n == 1) : # top 뉴스
                lis = list_1.find_elements_by_tag_name('li')
                for li in lis:
                    atag = li.find_element_by_tag_name('a')
                    print(atag.get_attribute('href'))
                    href.append(atag.get_attribute('href'))
            
            
            lis_2 = list_2.find_elements_by_tag_name('li')
            for li in lis_2:
                atag = li.find_element_by_tag_name('a')
                print(atag.get_attribute('href'))
                href.append(atag.get_attribute('href'))
           
            
        
        d = pd.DataFrame({'href':href})    
        print(d)
       
```

저장된 href를 통한 더 자세한 크롤링 작업 시작

```python
Title, Date, Content, Href = list(), list(), list(), list()

for i in range(0, len(d)):
    url = d.iloc[i]['href']
    print(url)
    raw = requests.get(url,headers={'User-Agent':'Mozilla/5.0'})
    html = BeautifulSoup(raw.text,'html.parser')
    title = html.select('.report_tit > h2')[0].text # 제목
    date = html.select('.report_tit > p')[0].text # 기사작성일 / 최종 수정일
    content_list = html.select('.con_area > p') # 본문
    print(title)
    print(date)
    content = ''
    for j in range(0,len(content_list)):
        content += content_list[j].text
    print(content)
    Title.append(title)
    Date.append(date)
    Content.append(content)
    Href.append(url)

df = pd.DataFrame({'title':Title,'date':Date,'content':Content,'href':Href})

df.to_excel('news_crawling.xlsx',index=False)    

```

print 결과 부분 

![image](https://user-images.githubusercontent.com/58428675/117567392-2c34ca80-b0f7-11eb-8327-671c61505d73.png)

엑셀파일에도 잘 저장된 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/58428675/117567445-656d3a80-b0f7-11eb-97e4-4799deea56a8.png)

[국회뉴스 ON 사이트]: http://www.naon.go.kr/assemblyNews/naon_today/list.do
